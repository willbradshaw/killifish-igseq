##############################################################################
## SNAKEFILE
## PIPELINE: Immunoglobulin sequencing analysis
## SUB-PIPELINE: Downstream analysis
## FILE: Snakefile
## AUTHOR: Will Bradshaw
##############################################################################
## Example run snakefile for Ig-Seq downstream-analysis pipeline
##############################################################################

#==============================================================================
# SETUP
#==============================================================================

#------------------------------------------------------------------------------
# Import config file
#------------------------------------------------------------------------------

configfile: "config_analysis.yaml"

#------------------------------------------------------------------------------
# Import python libraries
#------------------------------------------------------------------------------

import os, random, string
from functools import partial

#------------------------------------------------------------------------------
# Set wildcard constraints
#------------------------------------------------------------------------------

wildcard_constraints:
    dataset = "|".join(config["data_changeo"]),

#------------------------------------------------------------------------------
# Set up directories
#------------------------------------------------------------------------------

# Directories
input_dir  = "input_files"
out_dir    = "output_files"
log_dir    = "log_files"
env_dir    = "env"
script_dir = "scripts"

# Subdirectories for filtering
out_dir_filter = os.path.join(out_dir, "filter/{dataset}")
log_dir_filter = os.path.join(log_dir, "filter/{dataset}")

# Subdirectories for diversity analysis
out_dir_diversity = os.path.join(out_dir, "diversity/{divdir}/{dataset}")
log_dir_diversity = os.path.join(log_dir, "diversity/{divdir}/{dataset}")
out_dir_div_split = out_dir_diversity + "_split"
log_dir_div_split = log_dir_diversity + "_split"
out_dir_div_run   = os.path.join(out_dir_div_split, "run_{value}")
log_dir_div_run   = os.path.join(log_dir_div_split, "run_{value}")
out_dir_div_boot  = os.path.join(out_dir_div_split, "bootstrap_{replicate}")
log_dir_div_boot  = os.path.join(log_dir_div_split, "bootstrap_{replicate}")

# Subdirectories for IGOR analysis
out_dir_igor = os.path.join(out_dir, "igor")
log_dir_igor = os.path.join(log_dir, "igor")
out_dir_igor_split = os.path.join(out_dir_igor, "{dataset}_{field}")
log_dir_igor_split = os.path.join(log_dir_igor, "{dataset}_{field}")
out_dir_igor_run = os.path.join(out_dir_igor_split, "run_{value}")
log_dir_igor_run = os.path.join(log_dir_igor_split, "run_{value}")

#------------------------------------------------------------------------------
# Specify Conda environments
#------------------------------------------------------------------------------

env_r = os.path.join(env_dir, "igseq_analysis_r.yaml")
env_shazam = os.path.join(env_dir, "igseq_analysis_shazam.yaml")
env_shell = os.path.join(env_dir, "igseq_analysis_shell.yaml")
env_pygor = os.path.join(env_dir, "igseq_analysis_pygor.yaml")

#------------------------------------------------------------------------------
# Auxiliary functions for aggregating checkpoints
#------------------------------------------------------------------------------

class FormatDict(dict):
    """Dictionary class suitable for partial formatting."""
    def __missing__(self, key):
        return "{" + key + "}"

def get_format_fields(pattern):
    """Extract a list of format fields from a pattern string."""
    pattern_parsed = string.Formatter().parse(pattern)
    names = [f for _, f, _, _ in pattern_parsed if f is not None]
    return(names)

def format_partial_dict(pattern, dict):
    """Overwrite some format keys in a string while leaving others empty."""
    # Get list of fields from pattern
    fields = get_format_fields(pattern)
    # Set up formatting dictionary
    mapping = FormatDict()
    for k in dict.keys():
        if k in fields: mapping[k] = dict[k]
    # Format pattern string
    formatter = string.Formatter()
    out = formatter.vformat(pattern, (), mapping)
    return(out)

def format_partial(pattern, **fill):
    """Call format_partial_dict on a list of named arguments."""
    return(format_partial_dict(pattern, fill))

def wildcard_keys(wildcards):
    """Get a list of valid wildcard keys from a wildcards object."""
    return(wildcards._names.keys())
# TODO: Cut this function (unused)

def format_wildcards(pattern, wildcards, fields_fixed = {}):
    """Format a file pattern based on wildcards & specified fields."""
    d_wild = dict(wildcards)
    d_out = {**d_wild, **fields_fixed}
    return(format_partial_dict(pattern, d_out))

def extract_checkpoint(wildcards, # Snakemake wildcards object
        checkpoint_id, # Name of checkpoint rule
        checkpoint_pattern, # File pattern of checkpoint output files within dir
        target_field = None, # Field whose values to extract
        fields_fixed = {}, # Dictionary of key/value pairs to insert into patterns
        debug = False): # Print debug messages to workflow output?
    """Extract list of field values to use for checkpoint aggregation."""
    # 0. Subfunctions
    echo = lambda msg: print(msg) if debug else None
    # 1. Get path to checkpoint directory:
    checkpoint_obj = getattr(checkpoints, checkpoint_id)
    checkpoint_dir = checkpoint_obj.get(**wildcards).output[0]
    echo(f"Checkpoint directory: {checkpoint_dir}")
    # 2. Prepare filename pattern to use to search for checkpoint files
    check_pattern_full = os.path.join(checkpoint_dir, checkpoint_pattern)
    echo(f"Raw checkpoint pattern: {check_pattern_full}")
    check_pattern_filled = format_wildcards(check_pattern_full, wildcards, \
        fields_fixed)
    echo(f"Filled checkpoint pattern: {check_pattern_filled}")
    # 3. Get list of wildcard values to include in expansion
    if target_field is None:
        target_field = get_format_fields(check_pattern_filled)[0]
    echo(f"Target field: {target_field}")
    glob_obj = glob_wildcards(check_pattern_filled)
    out_values = sorted(getattr(glob_obj, target_field))
    echo(f"Target field values: {out_values}")
    return(out_values)

def aggregate_checkpoint(wildcards, # Snakemake wildcards object
        checkpoint_id, # Name of checkpoint rule
        checkpoint_pattern, # File pattern of checkpoint output files within dir
        expand_pattern, # File pattern to expand using target field values
        target_field = None, # Field whose values to extract
        expand_field = None, # Field to expand using target field values
        fields_fixed = {}, # Dictionary of key/value pairs to insert into patterns
        debug = False): # Print debug messages to workflow output?
    """Expand a filename pattern based on the output of a checkpoint."""
    # 0. Subfunctions
    echo = lambda msg: print(msg) if debug else None
    # 1. Get list of target field values
    target_values = extract_checkpoint(wildcards, checkpoint_id, \
        checkpoint_pattern, target_field, fields_fixed, debug)
    echo(f"Target field values: {target_values}")
    # 2. Prepare filename pattern to expand
    echo(f"Pattern to expand (raw): {expand_pattern}")
    expand_pattern_filled = format_wildcards(expand_pattern, wildcards, \
        fields_fixed)
    echo(f"Pattern to expand (formatted): {expand_pattern_filled}")
    # 3. Prepare expansion (for arbitrary field names)
    if expand_field is None:
        expand_field = get_format_fields(expand_pattern_filled)[0]
    echo(f"Field to expand: {expand_field}")
    expand_fn = partial(expand)
    expand_fn.keywords[expand_field] = target_values
    expand_out = expand_fn(expand_pattern_filled)
    echo(f"Final expansion: {expand_out}")
    return(expand_out)

# For any given checkpoint, define a partial function specifying
# checkpoint_id, checkpoint_pattern, target_field, fields_fixed (as needed).
# Then, when aggregation is needed, define a child function specifying
# expand_pattern, expand_field, and debug (as needed) to pass to the
# aggregation rule.

#==============================================================================
# DEFAULT RULE
#==============================================================================

divsets = config["diversity_datasets"]
diversity_path = out_dir_diversity + "_diversity-{divtype}-{optype}.tsv.gz"
kw_path = out_dir_diversity + "_kw-permute-pvalues.tsv.gz"
igor_path = os.path.join(out_dir_igor_split, "output/{outtype}.tsv.gz")

rule all:
    input:
        # Checkpoint tests
        expand(out_dir_diversity + "_outer-groups-aggregated.tsv.gz", \
            divdir = "segment_all", dataset = "gut_age"),
        expand(out_dir_diversity + "_bootstraps-aggregated.tsv.gz", \
            divdir = "segment_all", dataset = "gut_age"),
        # Diversity spectra
#        expand(diversity_path, divdir = "clone", \
#                dataset = "ageing", #divsets["clone"], \
#               divtype = ["alpha", "solo"], optype = "summarised"),
#        expand(diversity_path, divdir = "segment_all", \
#                dataset = divsets["segment_all"], \
#                divtype = ["alpha", "beta", "solo"], \
#                optype  = ["summarised", "bootstrapped"]),
#        expand(diversity_path, divdir = "segment_5plus", \
#                dataset = divsets["segment_5plus"], \
#                divtype = ["alpha", "beta", "solo"], \
#                optype  = ["summarised", "bootstrapped"]),
#        expand(diversity_path, divdir = "segment_4minus", \
#                dataset = divsets["segment_4minus"], \
#                divtype = ["alpha", "beta", "solo"], \
#                optype  = ["summarised", "bootstrapped"]),
#        # Diversity KW p-values
#        expand(kw_path, divdir = "clone", \
#                dataset = divsets["clone"]),
#        expand(kw_path, divdir = "segment_all", \
#                dataset = divsets["segment_all"]),
#        expand(kw_path, divdir = "segment_5plus", \
#                dataset = divsets["segment_5plus"]),
#        expand(kw_path, divdir = "segment_4minus", \
#                dataset = divsets["segment_4minus"]),
        # IGoR prep files
#        expand(os.path.join(out_dir_igor_split, "counts_all.csv"), \
#                dataset = config["igor_group_field"].keys(),
#                field = config["igor_group_field"].values()),
#        expand(os.path.join(out_dir_igor_split, "counts_all.csv"), \
#                dataset = config["igor_individuals"].keys(), field = "INDIVIDUAL"),
##        expand(igor_path, outtype = ["entropies", "segments", "indels"], \
##                dataset = config["igor_individuals"].keys(), field = "INDIVIDUAL"),
#        expand(igor_path, zip, outtype = ["entropies"] * len(config["igor_group_field"]),
#                dataset = config["igor_group_field"].keys(),
#                field = config["igor_group_field"].values()),
#        expand(igor_path, zip, outtype = ["indels"] * len(config["igor_group_field"]),
#                dataset = config["igor_group_field"].keys(),
#                field = config["igor_group_field"].values()),
#        expand(igor_path, zip, outtype = ["segments"] * len(config["igor_group_field"]),
#                dataset = config["igor_group_field"].keys(),
#                field = config["igor_group_field"].values()),

#==============================================================================
# FILTER INPUT DATA
#==============================================================================

rule filter_groups:
    """Exclude unwanted sequence groups from downstream analysis."""
    input: lambda wildcards: config["data_changeo"][wildcards.dataset]
    output: out_dir_filter + "_groups-filtered.tsv"
    log: log_dir_filter + "_filter-groups.dbg"
    conda: env_r
    params:
        filter_groups = lambda wildcards: config["filter_groups"][wildcards.dataset],
        keep_complement = lambda wildcards: config["keep_complement"][wildcards.dataset],
    script: os.path.join(script_dir, "filter_groups.R")

rule filter_ambiguous_segments:
    """Filter sequences with ambiguous segment calls from Change-O DB."""
    input: out_dir_filter + "_groups-filtered.tsv"
    output: out_dir_filter + "_{segments}-filtered.tsv"
    log: log_dir_filter + "_filter-{segments}.dbg"
    conda: env_r
    params:
        filter_V = lambda wildcards: "V" in wildcards.segments.upper(),
        filter_D = lambda wildcards: "D" in wildcards.segments.upper(),
        filter_J = lambda wildcards: "J" in wildcards.segments.upper(),
    script: os.path.join(script_dir, "filter_segments.R")

rule filter_clonesize_5plus:
    """Filter out smaller clones from a segment-filtered dataset."""
    input: out_dir_filter + "_{segments}-filtered.tsv"
    output: out_dir_filter + "_{segments}-filtered-5plus.tsv"
    log: log_dir_filter + "_filter-{segments}-clonesize-5plus.dbg"
    conda: env_r
    params:
        min_size = 5,
    script: os.path.join(script_dir, "filter_by_clonesize.R")

rule filter_clonesize_4minus:
    """Filter out smaller clones from a segment-filtered dataset."""
    input: out_dir_filter + "_{segments}-filtered.tsv"
    output: out_dir_filter + "_{segments}-filtered-4minus.tsv"
    log: log_dir_filter + "_filter-{segments}-clonesize-4minus.dbg"
    conda: env_r
    params:
        max_size = 4, # < 5
    script: os.path.join(script_dir, "filter_by_clonesize.R")

#==============================================================================
# DIVERSITY SPECTRA
#==============================================================================

clone_field_segments = "BEST_" + config["segment_diversity_type"] + "_CALL"
clone_fields = {"clone": "CLONE",
                "segment_all": clone_field_segments,
                "segment_5plus": clone_field_segments,
                "segment_4minus": clone_field_segments}

segment_div_prefix = out_dir_filter + "_" + \
    config["segment_diversity_type"] + "-filtered"
diversity_inputs = {"clone": out_dir_filter + "_groups-filtered.tsv",
                    "segment_all": segment_div_prefix + ".tsv",
                    "segment_5plus": segment_div_prefix + "-5plus.tsv",
                    "segment_4minus": segment_div_prefix + "-4minus.tsv"
}

#*****************************************************************************
# CHECKPOINT: Split sequence ID by outer-group ID
#*****************************************************************************

checkpoint split_db_outer_groups:
    """Split repertoire DB by outer grouping field."""
    input: lambda wildcards: diversity_inputs[wildcards.divdir]
    output: directory(out_dir_div_split)
    log: os.path.join(log_dir_div_split, "split-db.dbg")
    conda: env_shell
    params:
        group_within = lambda wildcards: config["group_within"][wildcards.dataset],
    shell:
        "mkdir -p {output} &> {log};"
        "ParseDb.py split -d {input} -f {params.group_within} "
        "--outdir {output} --outname {wildcards.dataset} &>> {log};"
        "for f in $(ls -d {output}/*); do echo Compressing ${{f}}... &>> {log};"
        "gzip -c ${{f}} > ${{f}}.gz 2>> {log}; rm ${{f}} &>> {log};"
        "echo Done. Renaming compressed file... &>> {log};"
        "g=${{f/{wildcards.dataset}_{params.group_within}-/}};"
        "mv ${{f}}.gz ${{g}}.gz &>> {log}; echo 'Done.' &>> {log};"
        "echo File list: $(ls -d {output}/*) &>> {log}; done"

# Output pattern
# All wildcards: {divdir}, {dataset}, {value}
file_pattern_outer_groups = "{value}.tsv.gz"
checkpt_pattern_outer_groups = os.path.join(out_dir_div_split, \
    file_pattern_outer_groups)

# Partial aggregation function
aggregate_checkpoint_outer_groups = partial(aggregate_checkpoint, \
    checkpoint_id = "split_db_outer_groups", \
    checkpoint_pattern = file_pattern_outer_groups) # No target_field or fields_fixed

# Complete aggregation function (for testing)
aggregate_checkpoint_outer_groups_test = partial( \
    aggregate_checkpoint_outer_groups, \
    expand_pattern = checkpt_pattern_outer_groups, \
    debug = True) # No expand_field

rule aggregate_db_outer_groups_test:
    """Test checkpoint by aggregating split outer-group files."""
    input: aggregate_checkpoint_outer_groups_test
    output: out_dir_diversity + "_outer-groups-aggregated.tsv.gz"
    log: log_dir_diversity + "_outer-groups-aggregated.dbg"
    conda: env_r
    script: os.path.join(script_dir, "merge_dbs.R")

#*****************************************************************************
# CHECKPOINT: Perform bootstrapping on each outer group and save each
# bootstrap replicate separately
#*****************************************************************************

# Output pattern
replicate_field = "replicate"
file_pattern_boot_reps = "bootstrap_{}.tsv.gz".format("{"+replicate_field+"}")
checkpt_pattern_boot_reps = os.path.join(out_dir_div_run, \
    file_pattern_boot_reps)
# All wildcards: {divdir}, {dataset}, {value}, {replicate}

# Checkpoint rule
checkpoint bootstrap_replicates:
    """Carry out bootstrapping & save separately for each replicate."""
    input: checkpt_pattern_outer_groups
    output: directory(out_dir_div_run)
    log: os.path.join(log_dir_div_split, "{value}-bootstrap-replicates.dbg")
    conda: env_r
    params:
        min_n = config["div_parameters"]["min_n"],
        max_n = config["div_parameters"]["max_n"],
        nboot = config["div_parameters"]["nboot"],
        clone_field = lambda wildcards: clone_fields[wildcards.divdir],
        group_within = lambda wildcards: config["group_within"][wildcards.dataset],
        group_between = lambda wildcards: config["group_between"][wildcards.dataset],
        replicate_field = replicate_field,
        out_pattern = lambda wildcards: file_pattern_boot_reps,
    script: os.path.join(script_dir, "compute_bootstraps_split.R")

# Partial aggregation function
aggregate_checkpoint_boot_reps = partial(aggregate_checkpoint, \
    checkpoint_id = "bootstrap_replicates", \
    checkpoint_pattern = file_pattern_boot_reps) # No target_field or fields_fixed

# Complete aggregation functions (for testing)
aggregate_checkpoint_boot_reps_test = partial( \
    aggregate_checkpoint_boot_reps, \
    expand_pattern = checkpt_pattern_boot_reps, \
    debug = True) # No expand_field
aggregate_checkpoint_outer_groups_boot_reps_test = partial( \
    aggregate_checkpoint_outer_groups, \
    expand_pattern = os.path.join(out_dir_div_split, "{value}-bootstraps-aggregated.tsv.gz"), \
    debug = True) # No expand_field

# Aggregation rules (for testing)
# NB: Likely to fail on low-memory machines unless you cut the number of replicates
rule aggregate_db_bootstrap_replicates_test_partial:
    """Test checkpoint by aggregating split bootstrap replicates."""
    input: aggregate_checkpoint_boot_reps_test
    output: os.path.join(out_dir_div_split, "{value}-bootstraps-aggregated.tsv.gz")
    log: os.path.join(log_dir_div_split, "{value}-bootstraps-aggregated.dbg")
    conda: env_r
    script: os.path.join(script_dir, "merge_dbs.R")
rule aggregate_db_bootstrap_replicates_test_full:
    """Test checkpoint by aggregating bootstrap replicates across outer groups."""
    input: aggregate_checkpoint_outer_groups_boot_reps_test
    output: out_dir_diversity + "_bootstraps-aggregated.tsv.gz"
    log: log_dir_diversity + "_bootstraps-aggregated.dbg"
    conda: env_r
    script: os.path.join(script_dir, "merge_dbs.R")

#-----------------------------------------------------------------------------
# Compute diversity metrics for each bootstrap replicate
#-----------------------------------------------------------------------------

# File patterns
diversity_pattern_boot_reps = checkpt_pattern_boot_reps.replace("bootstrap", \
    "diversity-{divtype}-bootstrapped")

rule diversity_table_bootstrapped:
    """Compute diversity scores for each bootstrap replicate."""
    input: checkpt_pattern_boot_reps
    output:
        alpha = diversity_pattern_boot_reps.format(divtype="alpha"),
        beta = diversity_pattern_boot_reps.format(divtype="beta"),
        gamma = diversity_pattern_boot_reps.format(divtype="gamma"),
        solo = diversity_pattern_boot_reps.format(divtype="solo"),
    log: os.path.join(log_dir_div_run, "diversity-bootstrapped_{replicate}.dbg")
    conda: env_r
    params:
        min_q = config["div_parameters"]["min_q"],
        max_q = config["div_parameters"]["max_q"],
        step_q = config["div_parameters"]["step_q"],
        clone_field = lambda wildcards: clone_fields[wildcards.divdir],
        group_within = lambda wildcards: config["group_within"][wildcards.dataset],
        group_between = lambda wildcards: config["group_between"][wildcards.dataset],
    script: os.path.join(script_dir, "compute_diversity.R")
# TODO: Aggregate and test

#rule summarise_diversity:
#    """Summarise bootstrapped diversity spectra."""
#    input: os.path.join(out_dir_div_run, "diversity-{divtype}-bootstrapped.tsv.gz")
#    output: os.path.join(out_dir_div_run, "diversity-{divtype}-summarised.tsv.gz")
#    log: os.path.join(log_dir_div_run, "summarise-diversity-{divtype}.dbg")
#    conda: env_r
#    params:
#        min_q = config["div_parameters"]["min_q"],
#        max_q = config["div_parameters"]["max_q"],
#        step_q = config["div_parameters"]["step_q"],
#        ci = config["div_parameters"]["ci"],
#        group_within = lambda wildcards: config["group_within"][wildcards.dataset],
#        group_between = lambda wildcards: config["group_between"][wildcards.dataset],
#    script: os.path.join(script_dir, "summarise_diversity.R")
#
#def aggregate_diversity_fn(wildcards):
#    """Aggregate split diversity files."""
#    # Get path to checkpoint directory
#    check_out = checkpoints.bootstrap_split_db.get(**wildcards).output[0]
#    # Prepare filename patterns (based on checkpoint rule output)
#    dataset, divdir = wildcards.dataset, wildcards.divdir
#    field = config["group_within"][wildcards.dataset]
#    path_base = os.path.join(check_out, "{dataset}_{field}-{{value}}.tsv")
#    path_fmt = path_base.format(dataset=dataset, divdir=divdir, field=field)
#    # Prepare and return output file list (based on aggregation rule input)
#    divtype,optype,value = wildcards.divtype, wildcards.optype, "{value}"
#    path_out_base = os.path.join(out_dir_div_run, \
#        "diversity-{divtype}-{optype}.tsv.gz")
#    path_out = path_out_base.format(dataset=dataset, divdir=divdir, \
#        value=value, divtype=divtype, optype=optype)
#    out_values = glob_wildcards(path_fmt).value
#    out_expand = expand(path_out, value = out_values)
#    return(out_expand)
#
#rule aggregate_diversity_rule:
#    """Aggregate split diversity files."""
#    input: aggregate_diversity_fn
#    output: out_dir_diversity + "_diversity-{divtype}-{optype}.tsv.gz"
#    log: log_dir_diversity + "_aggregate-diversity-{divtype}-{optype}.dbg"
#    conda: env_r
#    script: os.path.join(script_dir, "merge_dbs.R")
#
##-----------------------------------------------------------------------------
## Compute p-values for diversity differences using KW permutation tests
##-----------------------------------------------------------------------------
#
#rule clone_kruskal_wallis_permute:
#    """Run Kruskal-Wallis permutation tests on bootstrapped solo diversities."""
#    input: out_dir_diversity + "_diversity-solo-bootstrapped.tsv.gz"
#    output: out_dir_diversity + "_kw-permute-pvalues.tsv.gz"
#    log: log_dir_diversity + "_kruskal-wallis-permute.dbg"
#    conda: env_r
#    params:
#        n_permutes = config["div_parameters"]["n_permute"],
#        verbose = True,
#        group_within = lambda wildcards: config["group_within"][wildcards.dataset],
#        group_between = lambda wildcards: config["group_between"][wildcards.dataset],
#    threads: config["threads"]["kw"]
#    script: os.path.join(script_dir, "kruskal_wallis_permute.R")
#
##==============================================================================
## IGOR ANALYSIS
##==============================================================================
#
##-----------------------------------------------------------------------------
## Configure input files (other than Ig sequences)
##-----------------------------------------------------------------------------
#
#rule igor_extract_j_anchor:
#    """Generate IGoR J-anchor file from input auxiliary file."""
#    input: os.path.join(input_dir, "igor/j.aux")
#    output: os.path.join(out_dir_igor, "anchors/J.csv")
#    log: os.path.join(log_dir_igor, "extract_j_anchor.dbg")
#    conda: env_r
#    threads: config["threads"]["min"]
#    script: os.path.join(script_dir, "extract_j_anchors.R")
#
#rule igor_extract_v_anchor:
#    """Generate IGoR V-anchor file from IMGT-gapped V-sequences."""
#    input: os.path.join(input_dir, "igor/v.fasta")
#    output: os.path.join(out_dir_igor, "anchors/V.csv")
#    log: os.path.join(log_dir_igor, "extract_v_anchor.dbg")
#    conda: env_r
#    threads: config["threads"]["min"]
#    script: os.path.join(script_dir, "extract_v_anchors.R")
#
#rule igor_configure_model:
#    """Prepare default model parameter file for use by IGoR."""
#    input: os.path.join(input_dir, "igor/model_parms.txt")
#    output: os.path.join(out_dir_igor, "default_model.txt")
#    log: os.path.join(log_dir_igor, "configure_model.dbg")
#    conda: env_shell
#    shell:
#        "cp {input} {output} &> {log}"
#
#rule igor_igblast_make_db:
#    """Make BLAST databases for IgBLAST alignment"""
#    input: os.path.join(input_dir, "igor/{segment}_no_imgt.fasta")
#    output: os.path.join(out_dir_igor, "igblast/{segment}.nin")
#    log: os.path.join(log_dir_igor, "igblast/make_blast_db_{segment}.dbg")
#    conda: env_shell
#    params:
#        prefix = lambda wildcards, output: \
#            os.path.abspath(os.path.splitext(output[0])[0])
#    shell:
#        "makeblastdb -parse_seqids -dbtype nucl -in {input} "
#        "-out {params.prefix} &> {log}"
#
##-----------------------------------------------------------------------------
## Obtain nonfunctional semi-naive sequences for IGoR
##-----------------------------------------------------------------------------
#
#rule igor_collapse_clones:
#    """Collapse clones for SHM model construction."""
#    input: out_dir_filter + "_groups-filtered.tsv.gz"
#    output: os.path.join(out_dir_igor, "sequences/{dataset}_clones-collapsed-model.tsv.gz")
#    log: os.path.join(log_dir_igor, "sequences/{dataset}_collapse-clones-model.dbg")
#    threads: config["threads"]["max"]
#    conda: env_shazam
#    params:
#        exclude_na = True,
#        method = "mostCommon",
#    script: os.path.join(script_dir, "collapse_clones.R")
#
#rule igor_configure_db:
#    """Prepare Change-O DB for sequence extraction."""
#    input: os.path.join(out_dir_igor, "sequences/{dataset}_clones-collapsed-model.tsv.gz")
#    output: os.path.join(out_dir_igor, "sequences/{dataset}_igor-ready.tsv")
#    log: os.path.join(log_dir_igor, "sequences/{dataset}_configure-db.dbg")
#    conda: env_r
#    threads: config["threads"]["min"]
#    params:
#        individuals = lambda wildcards: config["igor_individuals"][wildcards.dataset],
#        group_field = lambda wildcards: config["igor_group_field"][wildcards.dataset],
#        groups = lambda wildcards: config["igor_groups"][wildcards.dataset],
#        collapsed = True,
#    script: os.path.join(script_dir, "configure_igor_db.R")
#
#rule igor_extract_sequences_initial:
#    """Extract clonal consensus sequences for functional checking."""
#    input: os.path.join(out_dir_igor, "sequences/{dataset}_igor-ready.tsv")
#    output: os.path.join(out_dir_igor, "sequences/{dataset}_igor-ready.fasta")
#    log: os.path.join(log_dir_igor, "sequences/{dataset}_extract-sequences-initial.dbg")
#    conda: env_shell
#    params:
#        prefix = lambda wildcards, output: \
#            os.path.abspath("_".join(output[0].split("_")[:-1])),
#        id_field = "SEQUENCE_ID",
#        seq_field = "SEQUENCE_OUT",
#        meta_fields = lambda wildcards: \
#            ["INDIVIDUAL", "REPLICATE", config["igor_group_field"][wildcards.dataset]],
#    shell:
#        "ConvertDb.py fasta -d {input} -o {output} --if {params.id_field} "
#        "--sf {params.seq_field} --mf {params.meta_fields} &> {log}"
#
#rule igor_igblast_align:
#    """Align consensus sequences to VDJ databases with IgBLAST"""
#    input:
#        seq = os.path.join(out_dir_igor, "sequences/{dataset}_igor-ready.fasta"),
#        v = os.path.join(out_dir_igor, "igblast/v.nin"),
#        d = os.path.join(out_dir_igor, "igblast/d.nin"),
#        j = os.path.join(out_dir_igor, "igblast/j.nin"),
#        aux = os.path.join(input_dir, "igor/j.aux"),
#    output: os.path.join(out_dir_igor, "igblast/{dataset}_align.fmt7")
#    log: os.path.join(log_dir_igor, "sequences/igor_{dataset}_igblast_align.dbg")
#    conda: env_shell
#    params:
#        v = lambda wildcards, input: os.path.splitext(input.v)[0],
#        d = lambda wildcards, input: os.path.splitext(input.d)[0],
#        j = lambda wildcards, input: os.path.splitext(input.j)[0],
#    threads: min(8, config["threads"]["med"]) # IgBLAST can't handle >8
#    shell:
#        "export IGDATA=${{CONDA_PREFIX}}/share/igblast; echo $IGDATA &> {log};"
#        "igblastn -ig_seqtype Ig -domain_system imgt -num_threads {threads} "
#        "-query {input.seq} -out {output} -germline_db_V {params.v} "
#        "-germline_db_J {params.j} -germline_db_D {params.d} "
#        "-auxiliary_data {input.aux} -outfmt '7 std qseq sseq btop' "
#        " &>> {log}"
#
#rule igor_changeo_make_db:
#    """Make sequence database from IgBLAST output with Change-O"""
#    input:
#        seq = os.path.join(out_dir_igor, "sequences/{dataset}_igor-ready.fasta"),
#        igblast = os.path.join(out_dir_igor, "igblast/{dataset}_align.fmt7"),
#        v = os.path.join(input_dir, "igor/v_no_imgt.fasta"),
#        d = os.path.join(input_dir, "igor/d_no_imgt.fasta"),
#        j = os.path.join(input_dir, "igor/j_no_imgt.fasta"),
#    output: os.path.join(out_dir_igor, "sequences/{dataset}_db-pass.tab")
#    conda: env_shell
#    log: os.path.join(log_dir_igor, "sequences/igor_{dataset}_make_changeo_db.dbg")
#    params:
#        prefix = lambda wildcards, output: \
#            os.path.abspath("_".join(output[0].split("_")[:-1])),
#    shell:
#        "MakeDb.py igblast --regions --scores --partial --asis-calls "
#        " --cdr3 -i {input.igblast} -s {input.seq} --outname {params.prefix} "
#        "-r {input.v} {input.d} {input.j} &> {log};"
#
#rule igor_changeo_split_functional:
#    """Identify and separate functional from nonfunctional sequences"""
#    input: os.path.join(out_dir_igor, "sequences/{dataset}_db-pass.tab")
#    output:
#        p = os.path.join(out_dir_igor, "sequences/{dataset}_FUNCTIONAL-T.tab"),
#        f = os.path.join(out_dir_igor, "sequences/{dataset}_FUNCTIONAL-F.tab"),
#    conda: env_shell
#    log: os.path.join(log_dir_igor, "sequences/igor_{dataset}_split_functional.dbg")
#    threads: config["threads"]["min"]
#    params:
#        prefix = lambda wildcards, output: \
#            os.path.abspath("_".join(output[0].split("_")[:-1])),
#    shell:
#        "ParseDb.py split -f FUNCTIONAL --outname {params.prefix} "
#        "-d {input} &> {log}; "
#        "if [[ ! -e {output.p} ]]; then touch {output.p} &>> {log}; fi; "
#        "if [[ ! -e {output.f} ]]; then touch {output.f} &>> {log}; fi"
#
##-----------------------------------------------------------------------------
## Split sequence DB by group ID for IGOR
##-----------------------------------------------------------------------------
#
#checkpoint igor_split_db:
#    """Split configured DB by the specified field."""
#    input: os.path.join(out_dir_igor, "sequences/{dataset}_FUNCTIONAL-F.tab"),
#    output: directory(os.path.join(out_dir_igor_split, "sequences"))
#    log: os.path.join(log_dir_igor_split, "split-db.dbg")
#    conda: env_shell
#    shell:
#        "mkdir -p {output};"
#        "ParseDb.py split -d {input} -f {wildcards.field} "
#        "--outdir {output} --outname {wildcards.dataset} &> {log}"
#
#def aggregate_igor_dbs(wildcards):
#    """Test checkpoint approach by aggregating split IGoR prep files."""
#    checkpoint_output = checkpoints.igor_split_db.get(**wildcards).output[0]
#    return expand(os.path.join(out_dir_igor_split, "sequences/{dataset}_{field}-{value}.tab"), \
#        value = glob_wildcards(os.path.join(checkpoint_output, "{dataset}_{field,[^-]+}-{value}.tab")).value,
#        dataset = wildcards.dataset, field = wildcards.field)
#
#rule aggregate_igor_collect_dbs:
#    """Test checkpoint approach by aggregating split IGoR prep files."""
#    input: aggregate_igor_dbs
#    output: os.path.join(out_dir_igor_split, "seqs_all_test.tab")
#    log: os.path.join(log_dir_igor_split, "db_aggregate.dbg")
#    conda: env_r
#    script: os.path.join(script_dir, "merge_dbs.R")
#
##-----------------------------------------------------------------------------
## Prepare input Ig sequences from split DB
##-----------------------------------------------------------------------------
#
#rule igor_extract_split_sequences:
#    """Extract split DB sequences into a FASTA file."""
#    input: os.path.join(out_dir_igor_split, "sequences/{dataset}_{field}-{value}.tab")
#    output: os.path.join(out_dir_igor_split, "sequences/{dataset}_{field}-{value}.fasta")
#    log: os.path.join(log_dir_igor_split, "{value}_extract_sequences.dbg")
#    conda: env_shell
#    params:
#        id_field = "SEQUENCE_ID",
#        seq_field = "SEQUENCE_INPUT",
#    shell:
#        "ConvertDb.py fasta -d {input} -o {output} --if {params.id_field} "
#        "--sf {params.seq_field} &> {log}"
#
#def aggregate_igor_seqs(wildcards):
#    """Test checkpoint approach by aggregating split IGoR prep files."""
#    checkpoint_output = checkpoints.igor_split_db.get(**wildcards).output[0]
#    return expand(os.path.join(out_dir_igor_split, "sequences/{dataset}_{field}-{value}.fasta"), \
#        value = glob_wildcards(os.path.join(checkpoint_output, "{dataset}_{field,[^-]+}-{value}.tab")).value,
#        dataset = wildcards.dataset, field = wildcards.field)
#
#rule aggregate_igor_collect_seqs:
#    """Test checkpoint approach by aggregating split IGoR prep files."""
#    input: aggregate_igor_seqs
#    output: os.path.join(out_dir_igor_split, "seqs_all_test.fasta")
#    log: os.path.join(log_dir_igor_split, "counts_aggregate.dbg")
#    shell:
#        "cat {input} > {output} 2> {log}"
#
##-----------------------------------------------------------------------------
## Infer generative models with IGoR
##-----------------------------------------------------------------------------
#
#rule igor_read_seqs:
#    """Read in sequences for alignment with IGoR."""
#    input: os.path.join(out_dir_igor_split, "sequences/{dataset}_{field}-{value}.fasta")
#    output: os.path.join(out_dir_igor_run, "aligns/indexed_sequences.csv")
#    log: os.path.join(log_dir_igor_run, "read_seqs.dbg")
#    conda: env_shell
#    params:
#        wd = lambda wildcards, output: out_dir_igor_run.format(\
#            dataset = wildcards.dataset, field = wildcards.field, \
#            value = wildcards.value),
#        igor = os.path.join(script_dir, "igor"),
#    threads: config["threads"]["min"]
#    shell:
#        "{params.igor} -set_wd {params.wd} -threads {threads} "
#        "-read_seqs {input} &> {log}"
#
#rule igor_align:
#    """Align sequences to VDJ databases with IGoR."""
#    input:
#        seqs = os.path.join(out_dir_igor_run, "aligns/indexed_sequences.csv"),
#        v = os.path.join(input_dir, "igor/v_no_imgt.fasta"),
#        d = os.path.join(input_dir, "igor/d_no_imgt.fasta"),
#        j = os.path.join(input_dir, "igor/j_no_imgt.fasta"),
#        janchor = os.path.join(out_dir_igor, "anchors/J.csv"),
#        vanchor = os.path.join(out_dir_igor, "anchors/V.csv"),
#    output:
#        info = os.path.join(out_dir_igor_run, "aligns/aligns_info.out"),
#        valn = os.path.join(out_dir_igor_run, "aligns/V_alignments.csv"),
#        daln = os.path.join(out_dir_igor_run, "aligns/D_alignments.csv"),
#        jaln = os.path.join(out_dir_igor_run, "aligns/J_alignments.csv"),
#    params:
#        wd = lambda wildcards, output: out_dir_igor_run.format(\
#            dataset = wildcards.dataset, field = wildcards.field, \
#            value = wildcards.value),
#        igor = os.path.join(script_dir, "igor"),
#    conda: env_shell
#    log: os.path.join(log_dir_igor_run, "align.dbg")
#    threads: config["threads"]["med"]
#    shell:
#        "{params.igor} -set_wd {params.wd} -threads {threads} "
#        "-set_genomic --V {input.v} --D {input.d} --J {input.j} "
#        "-set_CDR3_anchors --V {input.vanchor} --J {input.janchor} "
#        "-align --all &> {log}"
#
#rule igor_infer_model:
#    """Infer generative model from VDJ alignments."""
#    input:
#        aligns = os.path.join(out_dir_igor_run, "aligns/aligns_info.out"),
#        v = os.path.join(input_dir, "igor/v_no_imgt.fasta"),
#        d = os.path.join(input_dir, "igor/d_no_imgt.fasta"),
#        j = os.path.join(input_dir, "igor/j_no_imgt.fasta"),
#        janchor = os.path.join(out_dir_igor, "anchors/J.csv"),
#        vanchor = os.path.join(out_dir_igor, "anchors/V.csv"),
#        model = os.path.join(out_dir_igor, "default_model.txt"),
#    output:
#        marginals = os.path.join(out_dir_igor_run, "inference/final_marginals.txt"),
#        params = os.path.join(out_dir_igor_run, "inference/final_parms.txt"),
#        info = os.path.join(out_dir_igor_run, "inference/inference_info.out"),
#    params:
#        wd = lambda wildcards, output: out_dir_igor_run.format(\
#            dataset = wildcards.dataset, field = wildcards.field, \
#            value = wildcards.value),
#        igor = os.path.join(script_dir, "igor"),
#    conda: env_shell
#    log: os.path.join(log_dir_igor_run, "inference.dbg")
#    threads: config["threads"]["max"]
#    shell:
#        "{params.igor} -set_wd {params.wd} -threads {threads} "
#        "-set_custom_model {input.model} "
#        "-set_genomic --V {input.v} --D {input.d} --J {input.j} "
#        "-set_CDR3_anchors --V {input.vanchor} --J {input.janchor} "
#        "-infer &> {log}"
#
#rule igor_evaluate_model:
#    """Evaluate inferred model and generate output."""
#    input:
#        infers = os.path.join(out_dir_igor_run,"inference/inference_info.out"),
#        aligns = os.path.join(out_dir_igor_run, "aligns/aligns_info.out"),
#        model = os.path.join(out_dir_igor, "default_model.txt"),
#        marginals = os.path.join(out_dir_igor_run, "inference/final_marginals.txt"),
#        params = os.path.join(out_dir_igor_run, "inference/final_parms.txt"),
#    output:
#        evals = os.path.join(out_dir_igor_run, "evaluate/inference_info.out"),
#        scenarios = os.path.join(out_dir_igor_run,\
#            "output/best_scenarios_counts.csv"),
#        pgen = os.path.join(out_dir_igor_run, "output/Pgen_counts.csv"),
#        marginals = os.path.join(out_dir_igor_run,\
#            "evaluate/final_marginals.txt"),
#        parms = os.path.join(out_dir_igor_run,\
#            "evaluate/final_parms.txt"),
#    params:
#        wd = lambda wildcards, output: out_dir_igor_run.format(\
#            dataset = wildcards.dataset, field = wildcards.field, \
#            value = wildcards.value),
#        igor = os.path.join(script_dir, "igor"),
#    log: os.path.join(log_dir_igor_run, "evaluate.dbg")
#    conda: env_shell
#    threads: config["threads"]["med"]
#    shell:
#        "{params.igor} -set_wd {params.wd} -threads {threads} "
#        "-set_custom_model {input.params} {input.marginals} -evaluate "
#        "-output --scenarios 5 --Pgen --coverage VJ_gene &> {log}"
#
#def aggregate_igor(wildcards):
#    """Test checkpoint approach by aggregating split IGoR prep files."""
#    checkpoint_output = checkpoints.igor_split_db.get(**wildcards).output[0]
#    return expand(os.path.join(out_dir_igor_run, "output/best_scenarios_counts.csv"), \
#        value = glob_wildcards(os.path.join(checkpoint_output, "{dataset}_{field,[^-]+}-{value}.tab")).value,
#        dataset = wildcards.dataset, field = wildcards.field)
#
#rule aggregate_igor_collect:
#    """Test checkpoint approach by aggregating split IGoR prep files."""
#    input: aggregate_igor
#    output: os.path.join(out_dir_igor_split, "counts_all.csv")
#    log: os.path.join(log_dir_igor_split, "counts_aggregate.dbg")
#    shell:
#        "cat {input} > {output} &> {log}"
#
##-----------------------------------------------------------------------------
## Extract model information from output files
##-----------------------------------------------------------------------------
#
#rule extract_segment_dists:
#    """Extract V/D/J selection distributions from IGoR output."""
#    input:
#        marginals = os.path.join(out_dir_igor_run, \
#            "evaluate/final_marginals.txt"),
#        parms = os.path.join(out_dir_igor_run,\
#            "evaluate/final_parms.txt"),
#    output:
#        v = os.path.join(out_dir_igor_run, "output/segments/V.tsv"),
#        d = os.path.join(out_dir_igor_run, "output/segments/D.tsv"),
#        j = os.path.join(out_dir_igor_run, "output/segments/J.tsv"),
#        vj = os.path.join(out_dir_igor_run, "output/segments/VJ.tsv"),
#        vdj = os.path.join(out_dir_igor_run, "output/segments/VDJ.tsv"),
#    log: os.path.join(log_dir_igor_run, "extract-segment-dists.dbg")
#    conda: env_pygor
#    params:
#        script = os.path.join(script_dir, "igor_extract_segment_dists.py"),
#        indir = lambda wildcards, input: os.path.dirname(input.parms),
#        outdir = lambda wildcards, output: os.path.dirname(output.v),
#        id = lambda wildcards: wildcards.value,
#    threads: config["threads"]["min"]
#    shell:
#        "export PYTHONNOUSERSITE=True;"
#        "{params.script} {params.indir} {params.outdir} {params.id} &> {log}"
#
#rule extract_insertion_dists:
#    """Extract insertion distributions from IGoR output."""
#    input:
#        marginals = os.path.join(out_dir_igor_run,\
#            "evaluate/final_marginals.txt"),
#        parms = os.path.join(out_dir_igor_run,\
#            "evaluate/final_parms.txt"),
#    output:
#        os.path.join(out_dir_igor_run, "output/inserts.tsv"),
#    conda: env_pygor
#    log: os.path.join(log_dir_igor_run, "extract-insertion-dists.dbg")
#    params:
#        script = os.path.join(script_dir,\
#            "igor_extract_insert_dists.py"),
#        indir = lambda wildcards, input: os.path.dirname(input.parms),
#        outdir = lambda wildcards, output: os.path.dirname(output[0]),
#        id = lambda wildcards: wildcards.value,
#    threads: config["threads"]["min"]
#    shell:
#        "export PYTHONNOUSERSITE=True;"
#        "{params.script} {params.indir} {params.outdir} {params.id} &> {log}"
#
#rule extract_deletion_dists:
#    """Extract deletion distributions from IGoR output."""
#    input:
#        marginals = os.path.join(out_dir_igor_run,\
#            "evaluate/final_marginals.txt"),
#        parms = os.path.join(out_dir_igor_run,\
#            "evaluate/final_parms.txt"),
#    output:
#        os.path.join(out_dir_igor_run, "output/deletes.tsv"),
#    conda: env_pygor
#    log: os.path.join(log_dir_igor_run, "extract-deletion-dists.dbg")
#    params:
#        script = os.path.join(script_dir,\
#            "igor_extract_delete_dists.py"),
#        indir = lambda wildcards, input: os.path.dirname(input.parms),
#        outdir = lambda wildcards, output: os.path.dirname(output[0]),
#        id = lambda wildcards: wildcards.value,
#    threads: config["threads"]["min"]
#    shell:
#        "export PYTHONNOUSERSITE=True;"
#        "{params.script} {params.indir} {params.outdir} {params.id} &> {log}"
#
#rule extract_recombination_entropies:
#    """Extract recombination entropies from IGoR output."""
#    input:
#        marginals = os.path.join(out_dir_igor_run,\
#            "evaluate/final_marginals.txt"),
#        parms = os.path.join(out_dir_igor_run,\
#            "evaluate/final_parms.txt"),
#    output:
#        os.path.join(out_dir_igor_run, "output/entropies.tsv"),
#    conda: env_pygor
#    log: os.path.join(log_dir_igor_run, "extract-recombination-entropies.dbg")
#    params:
#        script = os.path.join(script_dir,\
#            "igor_extract_recombination_entropy.py"),
#        indir = lambda wildcards, input: os.path.dirname(input.parms),
#        outdir = lambda wildcards, output: os.path.dirname(output[0]),
#        id = lambda wildcards: wildcards.value,
#    threads: config["threads"]["min"]
#    shell:
#        "export PYTHONNOUSERSITE=True;"
#        "{params.script} {params.indir} {params.outdir} {params.id} &> {log}"
#
##-----------------------------------------------------------------------------
## Merge extracted TSVs across types and samples
##-----------------------------------------------------------------------------
#
#rule combine_segment_dists:
#    """Combine segment distributions into a single table."""
#    input:
#        os.path.join(out_dir_igor_run, "output/segments/V.tsv"),
#        os.path.join(out_dir_igor_run, "output/segments/D.tsv"),
#        os.path.join(out_dir_igor_run, "output/segments/J.tsv"),
#        os.path.join(out_dir_igor_run, "output/segments/VJ.tsv"),
#        os.path.join(out_dir_igor_run, "output/segments/VDJ.tsv"),
#    output: os.path.join(out_dir_igor_run, "output/segments.tsv")
#    log: os.path.join(log_dir_igor_run, "combine-segment-dists.dbg")
#    conda: env_r
#    script: os.path.join(script_dir, "merge_dbs.R")
#
#rule combine_indel_dists:
#    """Combine indel distributions into a single table."""
#    input:
#        os.path.join(out_dir_igor_run, "output/inserts.tsv"),
#        os.path.join(out_dir_igor_run, "output/deletes.tsv"),
#    output: os.path.join(out_dir_igor_run, "output/indels.tsv")
#    log: os.path.join(log_dir_igor_run, "combine-indel-dists.dbg")
#    conda: env_r
#    script: os.path.join(script_dir, "merge_dbs.R")
#
#def collect_igor_segments(wildcards):
#    """Collect segment DBs from different samples."""
#    checkpoint_output = checkpoints.igor_split_db.get(**wildcards).output[0]
#    return expand(os.path.join(out_dir_igor_run, "output/segments.tsv"), \
#        value = glob_wildcards(os.path.join(checkpoint_output, "{dataset}_{field,[^-]+}-{value}.tab")).value,
#        dataset = wildcards.dataset, field = wildcards.field)
#
#rule merge_segment_dists:
#    """Merge IGoR segment choice distributions across all groups."""
#    input: collect_igor_segments
#    output: os.path.join(out_dir_igor_split, "output/segments.tsv.gz")
#    log: os.path.join(log_dir_igor_split, "merge_segment_dists.dbg")
#    conda: env_r
#    threads: config["threads"]["min"]
#    script: os.path.join(script_dir, "merge_dbs.R")
#
#def collect_igor_indels(wildcards):
#    """Collect indel DBs from different samples."""
#    checkpoint_output = checkpoints.igor_split_db.get(**wildcards).output[0]
#    return expand(os.path.join(out_dir_igor_run, "output/indels.tsv"), \
#        value = glob_wildcards(os.path.join(checkpoint_output, "{dataset}_{field,[^-]+}-{value}.tab")).value,
#        dataset = wildcards.dataset, field = wildcards.field)
#
#rule merge_indel_dists:
#    """Merge IGoR indel choice distributions across all groups."""
#    input: collect_igor_indels
#    output: os.path.join(out_dir_igor_split, "output/indels.tsv.gz")
#    log: os.path.join(log_dir_igor_split, "merge_indel_dists.dbg")
#    conda: env_r
#    threads: config["threads"]["min"]
#    script: os.path.join(script_dir, "merge_dbs.R")
#
#def collect_igor_entropies(wildcards):
#    """Collect entropy DBs from different samples."""
#    checkpoint_output = checkpoints.igor_split_db.get(**wildcards).output[0]
#    return expand(os.path.join(out_dir_igor_run, "output/entropies.tsv"), \
#        value = glob_wildcards(os.path.join(checkpoint_output, "{dataset}_{field,[^-]+}-{value}.tab")).value,
#        dataset = wildcards.dataset, field = wildcards.field)
#
#rule merge_entropy_dists:
#    """Merge IGoR entropy choice distributions across all groups."""
#    input: collect_igor_entropies
#    output: os.path.join(out_dir_igor_split, "output/entropies.tsv.gz")
#    log: os.path.join(log_dir_igor_split, "merge_entropy_dists.dbg")
#    conda: env_r
#    threads: config["threads"]["min"]
#    script: os.path.join(script_dir, "merge_dbs.R")
#
## TODO: Get all this working again
## TODO: Integrate Michael's code

