##############################################################################
## SNAKEFILE
## PIPELINE: Immunoglobulin sequencing analysis
## SUB-PIPELINE: Pre-processing
## FILE: presto
## VERSION: 1.0
## AUTHOR: Will Bradshaw
## Date: 2018-08-08
##############################################################################
## Extract Ig-Seq sequences from UMI-tagged reads with pRESTO
##############################################################################

#-----------------------------------------------------------------------------
# Specify input and output directories
#-----------------------------------------------------------------------------

out_dir_presto = os.path.join(out_dir_preprocess, "presto")
log_dir_presto = os.path.join(log_dir_preprocess, "presto")
log_dir_pcount = os.path.join(log_dir_presto, "count")
out_dir_pcount = os.path.join(out_dir_presto, "count")

#-----------------------------------------------------------------------------
# Annotate, merge, and re-annotate reads
#-----------------------------------------------------------------------------

out_dir_pmake = os.path.join(out_dir_presto, "make")
log_dir_pmake = os.path.join(log_dir_presto, "make")
out_dir_pfilter = os.path.join(out_dir_presto, "filter")
log_dir_pfilter = os.path.join(log_dir_presto, "filter")

rule presto_add_fields:
    """Add sample information to sequence reads with ParseHeaders."""
    input: os.path.join(out_dir_setup, "reads/{sample}/reads_{read}.fastq")
    output: sample_path(out_dir_pmake, "R{read}_reheader.fastq")
    log: sample_path(log_dir_pmake, "add_fields_R{read}.dbg")
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        keys = lambda wildcards: " ".join(["SAMPLE"] + \
                list(map(str, config["samples"][wildcards.sample]["info"].keys())) + \
                list(map(str, config["info_all"].keys()))),
        values = lambda wildcards: " ".join([wildcards.sample] + \
                list(map(str, config["samples"][wildcards.sample]["info"].values())) + \
                list(map(str, config["info_all"].values()))),
    shell:
        "ParseHeaders.py add -f {params.keys} -u {params.values} -s {input} "
        "--outname {params.prefix} &> {log}"

rule presto_add_readcounts:
    """Add raw read number to read headers (for later)."""
    input:
        seq = sample_path(out_dir_pmake, "R{read}_reheader.fastq"),
        count = os.path.join(out_dir_setup, "reads/{sample}/reads_count.txt"),
    output: sample_path(out_dir_pmake, "R{read}_reheader_reheader.fastq")
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    log: sample_path(log_dir_pmake, "add_readcounts_R{read}.dbg")
    shell:
        'x=$(cat {input.count} |  tr -d "\n" | tr -d "\r");'
        "ParseHeaders.py add -f NREADS_RAW -u ${{x}} -s {input.seq} &> {log}; "

rule presto_label_replicates:
    """Label replicates by merging specified annotation fields."""
    input: sample_path(out_dir_pmake, "R{read}_reheader_reheader.fastq")
    output: sample_path(out_dir_pmake,\
        "R{read}_reheader_reheader_reheader.fastq")
    conda: env_main_preprocess
    log: sample_path(log_dir_pmake, "R{read}_label_replicates.dbg")
    threads: config["threads"]["min"]
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        fields = config["info_key_replicate"],
    shell:
        "ParseHeaders.py merge -f {params.fields} -k REPLICATE --act cat "
        "-s {input} --outname {params.prefix} &> {log} "

rule presto_label_individuals:
    """Label individuals by merging specified annotation fields."""
    input: sample_path(out_dir_pmake,\
        "R{read}_reheader_reheader_reheader.fastq")
    output: sample_path(out_dir_pmake,\
        "R{read}_reheader_reheader_reheader_reheader.fastq")
    log: sample_path(log_dir_pmake, "R{read}_label_individuals.dbg")
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        fields = config["info_key_individual"],
    shell:
        "ParseHeaders.py merge -f {params.fields} -k INDIVIDUAL --act cat "
        "-s {input} --outname {params.prefix} &> {log} "

rule presto_merge_samples:
    """Merge annotated FASTQ files into a single file pair."""
    input: lambda wildcards: expand(sample_path(out_dir_pmake, \
        "R{read}_reheader_reheader_reheader_reheader.fastq"), \
        read = wildcards.read, sample = list(config["samples"]))
    output: os.path.join(out_dir_pmake, "R{read}_merged.fastq")
    log: os.path.join(log_dir_pmake, "merge_seq_R{read}.fastq")
    threads: config["threads"]["min"]
    shell:
        "cat {input} > {output} 2> {log}"

#/////////////////////////////////////////////////////////////////////////////#

rule presto_data_table_raw:
    """Create a data table from merged, annotated, raw sequences."""
    input: os.path.join(out_dir_pmake, "R{read}_merged.fastq"),
    output: os.path.join(out_dir_pmake,\
       "R{read}_merged_headers.tab"),
    conda: env_main_preprocess
    log: os.path.join(log_dir_pmake, "R{read}_data_table.dbg")
    threads: config["threads"]["min"]
    params:
        logfields = "SAMPLE REPLICATE INDIVIDUAL NREADS_RAW"
    shell:
        "ParseHeaders.py table -s {input} -f {params.logfields} &>> {log}"

rule presto_count_raw:
    """Count reads and sequences after merging annotated raw reads."""
    input: os.path.join(out_dir_pmake,\
       "R1_merged_headers.tab"),
    output: os.path.join(out_dir_pcount, "count-raw.tsv")
    log: os.path.join(log_dir_pcount, "count-raw.dbg")
    threads: config["threads"]["min"]
    params:
        stage = "presto_raw",
        cluster_barcodes = config["presto_params"]["cluster_barcodes_pident"],
        cluster_sets = config["presto_params"]["cluster_sets_pident"],
        group_field = config["count_on"],
        aux = aux_dir_preprocess,
    conda: env_r_preprocess
    script: os.path.join(script_dir_preprocess, "count_changeo_db.R")

#/////////////////////////////////////////////////////////////////////////////#

#-----------------------------------------------------------------------------
# Split reads by replicate identity
#-----------------------------------------------------------------------------

out_dir_split = os.path.join(out_dir_presto, "split")
log_dir_split = os.path.join(log_dir_presto, "split")

rule presto_split_merged_R1:
    """Split merged R1 reads by replicate identity."""
    input: os.path.join(out_dir_pmake, "R1_merged.fastq")
    output: expand(os.path.join(out_dir_split, \
        "R1_split_REPLICATE-{rep}.fastq"),\
        rep = set(["".join(\
            [str(config["samples"][s]["info"][k]) for k in \
                config["info_key_replicate"].split(" ")]) for s in \
            list(config["samples"])]))
    log: os.path.join(log_dir_split, "R1_split-merged.dbg")
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    params:
        outdir = lambda wildcards, output: os.path.dirname(output[0]),
        outpref = "R1_split",
    shell:
        "SplitSeq.py group -s {input} -f REPLICATE --outdir {params.outdir} "
        "--outname {params.outpref} &> {log}"

rule presto_split_merged_R2:
    """Split merged R2 reads by replicate identity."""
    input: os.path.join(out_dir_pmake, "R2_merged.fastq")
    output: expand(os.path.join(out_dir_split, \
        "R2_split_REPLICATE-{rep}.fastq"),\
        rep = set(["".join(\
            [str(config["samples"][s]["info"][k]) for k in \
                config["info_key_replicate"].split(" ")]) for s in \
            list(config["samples"])]))
    log: os.path.join(log_dir_split, "R2_split-merged.dbg")
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    params:
        outdir = lambda wildcards, output: os.path.dirname(output[0]),
        outpref = "R2_split",
    shell:
        "SplitSeq.py group -s {input} -f REPLICATE --outdir {params.outdir} "
        "--outname {params.outpref} &> {log}"

#-----------------------------------------------------------------------------
# Quality-filter re-split reads
#-----------------------------------------------------------------------------

rule presto_filter_seq:
    """Filter input reads according to a quality threshold."""
    input: os.path.join(out_dir_split, "R{read}_split_REPLICATE-{rep}.fastq")
    output: os.path.join(out_dir_pfilter, "{rep}_R{read}_quality-pass.fastq")
    log: os.path.join(log_dir_pfilter, "{rep}_R{read}_FilterSeq.dbg")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        qual = config["presto_params"]["filter_qual"],
    conda: env_main_preprocess
    threads: config["threads"]["max"]
    shell:
        "FilterSeq.py quality -q {params.qual} --nproc {threads} "
        "-s {input} --outname {params.prefix} &> {log} "

#/////////////////////////////////////////////////////////////////////////////#
rule presto_merge_filtered:
    """Merge filtered FASTQ files into a single file pair."""
    input: lambda wildcards: expand(os.path.join(out_dir_pfilter, \
        "{rep}_R{read}_quality-pass.fastq"), \
        read = wildcards.read, rep = set(["".join(\
            [str(config["samples"][s]["info"][k]) for k in \
                config["info_key_replicate"].split(" ")]) for s in \
            list(config["samples"])]))
    threads: config["threads"]["min"]
    output: os.path.join(out_dir_pfilter, "R{read}_merged_filtered.fastq")
    log: os.path.join(log_dir_pfilter, "merge_filtered_R{read}.fastq")
    shell:
        "cat {input} > {output} 2> {log}"

rule presto_data_table_filter:
    """Create a data table from merged, annotated, raw sequences."""
    input: os.path.join(out_dir_pfilter, "R{read}_merged_filtered.fastq"),
    output: os.path.join(out_dir_pfilter,\
       "R{read}_merged_filtered_headers.tab"),
    conda: env_main_preprocess
    log: os.path.join(log_dir_pfilter, "R{read}_data_table.dbg")
    threads: config["threads"]["min"]
    params:
        logfields = "SAMPLE REPLICATE INDIVIDUAL NREADS_RAW"
    shell:
        "ParseHeaders.py table -s {input} -f {params.logfields} &>> {log}"

rule presto_count_filter:
    """Count reads and sequences after merging annotated raw reads."""
    input: os.path.join(out_dir_pfilter,\
       "R1_merged_filtered_headers.tab"),
    output: os.path.join(out_dir_pcount, "count-filtered.tsv")
    log: os.path.join(log_dir_pcount, "count-filtered.dbg")
    threads: config["threads"]["min"]
    params:
        stage = "presto_filter",
        cluster_barcodes = config["presto_params"]["cluster_barcodes_pident"],
        cluster_sets = config["presto_params"]["cluster_sets_pident"],
        group_field = config["count_on"],
        aux = aux_dir_preprocess,
    conda: env_r_preprocess
    script: os.path.join(script_dir_preprocess, "count_changeo_db.R")
#/////////////////////////////////////////////////////////////////////////////#

#-----------------------------------------------------------------------------
# Extract and mask primer/barcode sequences
#-----------------------------------------------------------------------------

out_dir_pmask = os.path.join(out_dir_presto, "mask")
log_dir_pmask = os.path.join(log_dir_presto, "mask")


rule presto_mask_primers_3prime:
    """Mask constant-region primers from 3' input reads"""
    input:
        seq = os.path.join(out_dir_pfilter, "{rep}_R2_quality-pass.fastq"),
        primers=os.path.join(out_dir_setup, "oligos/primers_c.fasta"),
    output: os.path.join(out_dir_pmask, "{rep}_R2_primers-pass.fastq")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
    log: os.path.join(log_dir_pmask, "{rep}_R2_MaskPrimers.dbg")
    conda: env_main_preprocess
    threads: config["threads"]["max"]
    shell:
        "MaskPrimers.py score --mode cut --nproc {threads} --start 0 "
        "-s {input.seq} -p {input.primers} --outname {params.prefix} &> {log} "

rule presto_mask_primers_5prime_m1s:
    """Mask M1s TSA primer from 5' input reads"""
    input:
        seq = os.path.join(out_dir_pfilter, "{rep}_R1_quality-pass.fastq"),
        primers=os.path.join(out_dir_setup, "oligos/primers_tsa.fasta"),
    output: os.path.join(out_dir_pmask, "{rep}_R1_partial_primers-pass.fastq")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
    log: os.path.join(log_dir_pmask, "{rep}_R1_partial_MaskPrimers.dbg")
    conda: env_main_preprocess
    threads: config["threads"]["max"]
    shell:
        "MaskPrimers.py score --mode cut --nproc {threads} "
        "-s {input.seq} -p {input.primers} --outname {params.prefix} &> {log} "

rule presto_mask_primers_5prime_umi:
    """Extract UMI barcodes from primer-masked 5' input reads"""
    input:
        seq = os.path.join(out_dir_pmask, "{rep}_R1_partial_primers-pass.fastq"),
        tsa=os.path.join(out_dir_setup, "oligos/tsa.fasta"),
    output: os.path.join(out_dir_pmask, "{rep}_R1_primers-pass.fastq")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        start = config["presto_params"]["tsa_barcode_length"],
        error = config["presto_params"]["tsa_max_error"],
    log: os.path.join(log_dir_pmask, "{rep}_R1_MaskPrimers.dbg")
    conda: env_main_preprocess
    threads: config["threads"]["max"]
    shell:
        "MaskPrimers.py score --mode cut --nproc {threads} --barcode "
        "--start {params.start} --maxerror {params.error} -p {input.tsa} "
        "-s {input.seq} --outname {params.prefix} &> {log} "
# TODO: Why does only the third masking step have a --maxerror parameter?

#/////////////////////////////////////////////////////////////////////////////#
rule presto_merge_masked:
    """Merge masked FASTQ files into a single file pair."""
    input: lambda wildcards: expand(os.path.join(out_dir_pmask, \
        "{rep}_R{read}_primers-pass.fastq"), \
        read = wildcards.read, rep = set(["".join(\
            [str(config["samples"][s]["info"][k]) for k in \
                config["info_key_replicate"].split(" ")]) for s in \
            list(config["samples"])]))
    threads: config["threads"]["min"]
    output: os.path.join(out_dir_pmask, "R{read}_merged_primers-pass.fastq")
    log: os.path.join(log_dir_pmask, "merge_masked_R{read}.fastq")
    shell:
        "cat {input} > {output} 2> {log}"

rule presto_data_table_mask:
    """Create a data table from UMI-masked reads."""
    input: os.path.join(out_dir_pmask, "R1_merged_primers-pass.fastq")
    output: os.path.join(out_dir_pmask, "R1_merged_primers-pass_headers.tab")
    conda: env_main_preprocess
    log: os.path.join(log_dir_pmask, "R1_data_table.dbg")
    threads: config["threads"]["min"]
    params:
        logfields = "SAMPLE REPLICATE INDIVIDUAL NREADS_RAW"
    shell:
        "ParseHeaders.py table -s {input} -f {params.logfields} &>> {log}"

rule presto_count_mask:
    """Count reads and sequences from masked read headers."""
    input: os.path.join(out_dir_pmask, "R1_merged_primers-pass_headers.tab")
    output: os.path.join(out_dir_pcount, "count-masked.tsv")
    log: os.path.join(log_dir_pcount, "count-masked.dbg")
    threads: config["threads"]["min"]
    params:
        stage = "presto_mask",
        cluster_barcodes = config["presto_params"]["cluster_barcodes_pident"],
        cluster_sets = config["presto_params"]["cluster_sets_pident"],
        group_field = config["count_on"],
        aux = aux_dir_preprocess,
    conda: env_r_preprocess
    script: os.path.join(script_dir_preprocess, "count_changeo_db.R")
#/////////////////////////////////////////////////////////////////////////////#

#-----------------------------------------------------------------------------
# Cluster and correct barcodes
#-----------------------------------------------------------------------------

out_dir_pcorrect = os.path.join(out_dir_presto, "correct")
log_dir_pcorrect = os.path.join(log_dir_presto, "correct")

# TODO: EstimateError?

rule presto_cluster_barcodes:
    """Cluster 5' reads by barcode to correct barcode errors."""
    input: os.path.join(out_dir_pmask, "{rep}_R1_primers-pass.fastq")
    output: os.path.join(out_dir_pcorrect, "{rep}_R1_cluster-pass.fastq")
    log: os.path.join(log_dir_pcorrect, "{rep}_R1_cluster-barcodes.dbg")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        ident = config["presto_params"]["cluster_barcodes_pident"],
        cluster = str(config["presto_params"]["cluster_umis"]),
    conda: env_main_preprocess
    threads: config["threads"]["max"]
    shell:
        "if [[ {params.cluster} == True ]]; then "
        "ClusterSets.py barcode -f BARCODE -k CLUSTER --cluster cd-hit-est "
        "--prefix B --ident {params.ident} --nproc {threads} "
        "-s {input} --outname {params.prefix} &> {log}; "
        "else cp {input} {output} &> {log}; fi "

rule presto_cluster_sets:
    """Cluster sequences within barcode sets to split barcode collisions."""
    input: os.path.join(out_dir_pcorrect, "{rep}_R1_cluster-pass.fastq")
    output: os.path.join(out_dir_pcorrect, \
            "{rep}_R1_cluster-pass_cluster-pass.fastq")
    log: os.path.join(log_dir_pcorrect, "{rep}_R1_cluster_sets.dbg")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        ident = config["presto_params"]["cluster_sets_pident"],
        cluster = str(config["presto_params"]["cluster_umis"]),
    conda: env_main_preprocess
    threads: config["threads"]["mor"]
    shell:
        "if [[ {params.cluster} == True ]]; then "
        "ClusterSets.py set -f CLUSTER -k CLUSTER --cluster vsearch "
        "--prefix S --ident {params.ident} --nproc {threads} "
        "-s {input} --outname {params.prefix} &> {log}; "
        "else cp {input} {output} &> {log}; fi"

rule presto_parse_clusters:
    """Simplify header information from clustered reads."""
    input: os.path.join(out_dir_pcorrect, \
            "{rep}_R1_cluster-pass_cluster-pass.fastq")
    output: os.path.join(out_dir_pcorrect, \
            "{rep}_R1_cluster-pass_reheader.fastq")
    log: os.path.join(log_dir_pcorrect, "{rep}_R1_parse_clusters.dbg")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        cluster = str(config["presto_params"]["cluster_umis"]),
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    shell:
        "if [[ {params.cluster} == True ]]; then "
        "ParseHeaders.py collapse -s {input} -f CLUSTER --act cat "
        " --outname {params.prefix} &> {log}; "
        "else ParseHeaders.py copy -s {input} -f BARCODE -k CLUSTER "
        " --outname {params.prefix} &> {log}; fi"

rule presto_disambiguate_clusters:
    """Make cluster annotations unique across all replicates."""
    input: os.path.join(out_dir_pcorrect, \
            "{rep}_R1_cluster-pass_reheader.fastq")
    output: os.path.join(out_dir_pcorrect, \
            "{rep}_R1_cluster-pass_reheader_reheader.fastq")
    log: os.path.join(log_dir_pcorrect, "{rep}_R1_disambiguate_clusters.dbg")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        cluster = str(config["presto_params"]["cluster_umis"]),
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    shell:
        "if [[ {params.cluster} == True ]]; then "
        "ParseHeaders.py merge -f REPLICATE CLUSTER -k RCLUSTER --act set "
        "-s {input} --outname {params.prefix} &> {log}; "
        "else cp {input} {output} &> {log}; fi"

rule copy_reads_3prime:
    """Copy 3-prime reads (without UMI) to correction directory."""
    input: os.path.join(out_dir_pmask, "{rep}_R2_primers-pass.fastq")
    output: os.path.join(out_dir_pcorrect, "{rep}_R2_split-masked.fastq"),
    log: os.path.join(log_dir_pcorrect, "{rep}_copy_reads_3prime.dbg")
    threads: config["threads"]["min"]
    shell: "cp {input} {output} &> {log}"

rule presto_unify_clusters:
    """Unify barcode (UMI) and cluster annotations across read pairs"""
    input:
        r1 = os.path.join(out_dir_pcorrect, \
            "{rep}_R1_cluster-pass_reheader_reheader.fastq"),
        r2 = os.path.join(out_dir_pcorrect, "{rep}_R2_split-masked.fastq"),
    output:
        r1 = os.path.join(out_dir_pcorrect,\
            "{rep}_R1_cluster-pass_reheader_reheader_pair-pass.fastq"),
        r2 = os.path.join(out_dir_pcorrect,\
            "{rep}_R2_split-masked_pair-pass.fastq"),
    threads: config["threads"]["min"]
    conda: env_main_preprocess
    log: os.path.join(log_dir_pcorrect, "{rep}_R12_PairSeq1.dbg")
    params:
        coord = config["presto_params"]["pairseq_coord"],
    shell:
        "PairSeq.py -1 {input.r1} -2 {input.r2} --1f BARCODE CLUSTER RCLUSTER "
        "--coord {params.coord} &> {log}"

rule presto_rename_unified_clusters:
    """Copy unified cluster files to new paths."""
    input:
        r1 = os.path.join(out_dir_pcorrect,\
            "{rep}_R1_cluster-pass_reheader_reheader_pair-pass.fastq"),
        r2 = os.path.join(out_dir_pcorrect,\
            "{rep}_R2_split-masked_pair-pass.fastq"),
    output:
        r1 = os.path.join(out_dir_pcorrect, "{rep}_R1_unified_pair-pass.fastq"),
        r2 = os.path.join(out_dir_pcorrect, "{rep}_R2_unified_pair-pass.fastq"),
    threads: config["threads"]["min"]
    log: os.path.join(log_dir_pcorrect, "{rep}_R12_rename_unified_clusters.dbg")
    shell:
        "cp {input.r1} {output.r1} &> {log}; "
        "cp {input.r2} {output.r2} &>> {log} "

#/////////////////////////////////////////////////////////////////////////////#
rule presto_merge_replicates:
    """Merge clustered FASTQ files into a single file pair."""
    input: lambda wildcards: expand(os.path.join(out_dir_pcorrect, \
        "{rep}_R{read}_unified_pair-pass.fastq"), \
        read = wildcards.read, rep = set(["".join(\
            [str(config["samples"][s]["info"][k]) for k in \
                config["info_key_replicate"].split(" ")]) for s in \
            list(config["samples"])]))
    threads: config["threads"]["min"]
    output: os.path.join(out_dir_pcorrect, "R{read}_merged_clustered.fastq")
    log: os.path.join(log_dir_pmake, "merge_replicates_R{read}.fastq")
    shell:
        "cat {input} > {output} 2> {log}"

rule presto_data_table_correct:
    """Create a data table from UMI-clustered reads."""
    input: os.path.join(out_dir_pcorrect, "R1_merged_clustered.fastq")
    output: os.path.join(out_dir_pcorrect, "R1_merged_clustered_headers.tab")
    conda: env_main_preprocess
    log: os.path.join(log_dir_pcorrect, "R1_data_table.dbg")
    threads: config["threads"]["min"]
    params:
        logfields = "SAMPLE REPLICATE INDIVIDUAL NREADS_RAW"
    shell:
        "ParseHeaders.py table -s {input} -f {params.logfields} &>> {log}"

rule presto_count_corrected:
    """Count reads and sequences from masked read headers."""
    input: os.path.join(out_dir_pcorrect, "R1_merged_clustered_headers.tab")
    output: os.path.join(out_dir_pcount, "count-corrected.tsv")
    log: os.path.join(log_dir_pcount, "count-corrected.dbg")
    threads: config["threads"]["min"]
    params:
        stage = "presto_correct",
        cluster_barcodes = config["presto_params"]["cluster_barcodes_pident"],
        cluster_sets = config["presto_params"]["cluster_sets_pident"],
        group_field = config["count_on"],
        aux = aux_dir_preprocess,
    conda: env_r_preprocess
    script: os.path.join(script_dir_preprocess, "count_changeo_db.R")
#/////////////////////////////////////////////////////////////////////////////#

#-----------------------------------------------------------------------------
# Assemble molecular identifier groups and construct consensus
#-----------------------------------------------------------------------------

out_dir_pconsensus = os.path.join(out_dir_presto, "consensus")
log_dir_pconsensus = os.path.join(log_dir_presto, "consensus")

rule presto_build_consensus:
    """Aggregate UMI groups and build consensus for each group"""
    input: os.path.join(out_dir_pcorrect, "{rep}_R{read}_unified_pair-pass.fastq")
    output: os.path.join(out_dir_pconsensus, "{rep}_R{read}_unified_consensus-pass.fastq")
    log: os.path.join(log_dir_pconsensus, "{rep}_R{read}_build_consensus.dbg")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        error = config["presto_params"]["build_consensus_max_error"],
        gap = config["presto_params"]["build_consensus_max_gap"],
        copy_fields = " ".join(set(config["info_all"]).union(list(itertools.chain(
            *[list(config["samples"][s]["info"]) for s in list(config["samples"])])))),
        copy_acts = " ".join(["set"] * len(set(config["info_all"]).union(list(itertools.chain(
            *[list(config["samples"][s]["info"]) for s in list(config["samples"])]))))),
    conda: env_main_preprocess
    threads: config["threads"]["mor"]
    shell:
        "BuildConsensus.py --nproc {threads} --bf RCLUSTER --cf RCLUSTER CLUSTER BARCODE "
        "INDIVIDUAL REPLICATE NREADS_RAW {params.copy_fields} --act set set set set set set "
        "{params.copy_acts} --maxerror {params.error} --maxgap {params.gap} "
        "-s {input} --outname {params.prefix} &> {log}; "

rule presto_unify_migs:
    """Filter unpaired consensus sequences from both files."""
    input:
        r1 = os.path.join(out_dir_pconsensus, "{rep}_R1_unified_consensus-pass.fastq"),
        r2 = os.path.join(out_dir_pconsensus, "{rep}_R2_unified_consensus-pass.fastq"),
    output:
        r1 = os.path.join(out_dir_pconsensus, "{rep}_R1_unified_consensus-pass_pair-pass.fastq"),
        r2 = os.path.join(out_dir_pconsensus, "{rep}_R2_unified_consensus-pass_pair-pass.fastq"),
    threads: config["threads"]["min"]
    log: os.path.join(log_dir_pconsensus, "{rep}_R12_unify_migs.dbg")
    conda: env_main_preprocess
    shell:
        "PairSeq.py -1 {input.r1} -2 {input.r2} --coord presto &> {log}"

#/////////////////////////////////////////////////////////////////////////////#
rule presto_merge_consensus:
    """Merge FASTQ consensus files into a single file pair."""
    input: lambda wildcards: expand(os.path.join(out_dir_pconsensus, \
        "{rep}_R{read}_unified_consensus-pass_pair-pass.fastq"), \
        read = wildcards.read, rep = sorted(set(["".join(\
            [str(config["samples"][s]["info"][k]) for k in \
                config["info_key_replicate"].split(" ")]) for s in \
            list(config["samples"])])))
    threads: config["threads"]["min"]
    output: os.path.join(out_dir_pconsensus, "R{read}_merged_consensus.fastq")
    log: os.path.join(log_dir_pconsensus, "merge_replicates_R{read}.fastq")
    shell:
        "cat {input} > {output} 2> {log}"
    # Retain these files to avoid complete rerun if V-DB changes

rule presto_data_table_consensus:
    """Create a data table from cluster consensus reads."""
    input: os.path.join(out_dir_pconsensus, \
            "R1_merged_consensus.fastq")
    output: os.path.join(out_dir_pconsensus, \
            "R1_merged_consensus_headers.tab"),
    conda: env_main_preprocess
    log: os.path.join(log_dir_pconsensus, "R1_data_table.dbg")
    threads: config["threads"]["min"]
    params:
        logfields = "SAMPLE REPLICATE INDIVIDUAL CONSCOUNT NREADS_RAW"
    shell:
        "ParseHeaders.py table -s {input} -f {params.logfields} &>> {log}"

rule presto_count_consensus:
    """Count reads and sequences from cluster consensus reads."""
    input: os.path.join(out_dir_pconsensus, \
            "R1_merged_consensus_headers.tab"),
    output: os.path.join(out_dir_pcount, "count-consensus.tsv")
    log: os.path.join(log_dir_pcount, "count-consensus.dbg")
    threads: config["threads"]["min"]
    params:
        stage = "presto_consensus",
        cluster_barcodes = config["presto_params"]["cluster_barcodes_pident"],
        cluster_sets = config["presto_params"]["cluster_sets_pident"],
        group_field = config["count_on"],
        aux = aux_dir_preprocess,
    conda: env_r_preprocess
    script: os.path.join(script_dir_preprocess, "count_changeo_db.R")
#/////////////////////////////////////////////////////////////////////////////#

#-----------------------------------------------------------------------------
# Merge read pairs into single sequences and collapse identical sequences
#-----------------------------------------------------------------------------

out_dir_pmerge = os.path.join(out_dir_presto, "merge")
log_dir_pmerge = os.path.join(log_dir_presto, "merge")

rule presto_assemble_pairs:
    """Merge consensus read pairs into complete sequences"""
    input:
        r1 = os.path.join(out_dir_pconsensus, "{rep}_R1_unified_consensus-pass_pair-pass.fastq"),
        r2 = os.path.join(out_dir_pconsensus, "{rep}_R2_unified_consensus-pass_pair-pass.fastq"),
        v = os.path.join(out_dir_setup, "vdj/v.fasta"),
    output:
        seq = os.path.join(out_dir_pmerge, "{rep}_R12_assemble-pass.fastq"),
        log = os.path.join(out_dir_pmerge, "{rep}_R12_assemble-pass.log"),
        tab = os.path.join(out_dir_pmerge, "{rep}_R12_assemble-pass_table.tab"),
    log: os.path.join(log_dir_pmerge, "{rep}_R12_assemble_pairs.dbg")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output["seq"].split("_")[:-1])),
        logfields = "ID FIELDS1 LENGTH OVERLAP ERROR PVALUE",
        copy_fields = " ".join(set(config["info_all"]).union(list(itertools.chain(
            *[list(config["samples"][s]["info"]) for s in list(config["samples"])])))),
    conda: env_main_preprocess
    threads: config["threads"]["mor"]
    shell:
        "AssemblePairs.py sequential --coord presto --scanrev --aligner blastn"
        " --nproc {threads} --rc tail --1f CONSCOUNT RCLUSTER CLUSTER BARCODE "
        "BARCODE_COUNT SAMPLE REPLICATE INDIVIDUAL NREADS_RAW {params.copy_fields} "
        "-1 {input.r1} -2 {input.r2} -r {input.v} "
        "--outname {params.prefix} --log {output.log} &> {log};"
        "ParseLog.py -l {output.log} -f {params.logfields} &>> {log} "
    # Keeping pRESTO log/table here for now - contains useful info not in headers

#/////////////////////////////////////////////////////////////////////////////#
rule presto_merge_merged:
    """Merge merged IgH sequence files into a single file."""
    input: lambda wildcards: expand(\
        os.path.join(out_dir_pmerge, "{rep}_R12_assemble-pass.fastq"),\
        rep = sorted(set(["".join(\
            [str(config["samples"][s]["info"][k]) for k in \
                config["info_key_replicate"].split(" ")]) for s in \
            list(config["samples"])])))
    output: os.path.join(out_dir_pmerge, "R12_merged_merged.fastq")
    threads: config["threads"]["min"]
    log: os.path.join(log_dir_pmerge, "merge_replicates_R12.fastq")
    shell:
        "cat {input} > {output} 2> {log}"
    # Retain these files to avoid complete rerun if V-DB changes

rule presto_data_table_merged:
    """Create a data table from merged IgH sequences."""
    input: os.path.join(out_dir_pmerge, "R12_merged_merged.fastq"),
    output: os.path.join(out_dir_pmerge, "R12_merged_merged_headers.tab"),
    conda: env_main_preprocess
    log: os.path.join(log_dir_pmerge, "R1_data_table.dbg")
    threads: config["threads"]["min"]
    params:
        logfields = "SAMPLE REPLICATE INDIVIDUAL CONSCOUNT NREADS_RAW"
    shell:
        "ParseHeaders.py table -s {input} -f {params.logfields} &>> {log}"

rule presto_count_merged:
    """Count reads and sequences from merged IgH sequences."""
    input: os.path.join(out_dir_pmerge, "R12_merged_merged_headers.tab"),
    output: os.path.join(out_dir_pcount, "count-merged.tsv")
    log: os.path.join(log_dir_pcount, "count-merged.dbg")
    threads: config["threads"]["min"]
    params:
        stage = "presto_merged",
        cluster_barcodes = config["presto_params"]["cluster_barcodes_pident"],
        cluster_sets = config["presto_params"]["cluster_sets_pident"],
        group_field = config["count_on"],
        aux = aux_dir_preprocess,
    conda: env_r_preprocess
    script: os.path.join(script_dir_preprocess, "count_changeo_db.R")
#/////////////////////////////////////////////////////////////////////////////#

#-----------------------------------------------------------------------------
# Collapse identical sequences within each replicate
#-----------------------------------------------------------------------------

rule presto_collapse_seq_replicates:
    """Collapse identical seqs and sum abundances over duplicates."""
    input: os.path.join(out_dir_pmerge, "{rep}_R12_assemble-pass.fastq"),
    output: os.path.join(out_dir_pmerge, "{rep}_R12_collapse-unique.fastq")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
        gap = config["presto_params"]["collapse_seq_max_gap"],
        copy_fields = " ".join(set(config["info_all"]).union(list(itertools.chain(
            *[list(config["samples"][s]["info"]) for s in list(config["samples"])])))),
        copy_acts = " ".join(["set"] * len(set(config["info_all"]).union(list(itertools.chain(
            *[list(config["samples"][s]["info"]) for s in list(config["samples"])]))))),
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    log: os.path.join(log_dir_pmerge, "{rep}_R12_collapse_seq.dbg")
    shell:
        "CollapseSeq.py --inner --cf CONSCOUNT RCLUSTER CLUSTER BARCODE BARCODE_COUNT "
        "SAMPLE REPLICATE INDIVIDUAL NREADS_RAW {params.copy_fields} "
        "--act sum set set set set set set set set {params.copy_acts} -n {params.gap} "
        "-s {input} --outname {params.prefix} &> {log} "

rule presto_merge_collapsed:
    """Merge collapsed IgH sequence files into a single file."""
    input: lambda wildcards: expand(\
        os.path.join(out_dir_pmerge, "{rep}_R12_collapse-unique.fastq"),\
        rep = sorted(set(["".join(\
            [str(config["samples"][s]["info"][k]) for k in \
                config["info_key_replicate"].split(" ")]) for s in \
            list(config["samples"])])))
    output: os.path.join(out_dir_pmerge, "R12_merged_collapse-unique.fastq")
    threads: config["threads"]["min"]
    log: os.path.join(log_dir_pmerge, "merge_replicates_R12.fastq")
    shell:
        "cat {input} > {output} 2> {log}"

#/////////////////////////////////////////////////////////////////////////////#
rule presto_data_table_collapsed:
    """Create a data table from collapsed IgH sequences."""
    input: os.path.join(out_dir_pmerge, "R12_merged_collapse-unique.fastq")
    output: os.path.join(out_dir_pmerge, "R12_merged_collapse-unique_headers.tab")
    conda: env_main_preprocess
    log: os.path.join(log_dir_pmerge, "R1_data_table_collapse.dbg")
    threads: config["threads"]["min"]
    params:
        logfields = "SAMPLE REPLICATE INDIVIDUAL CONSCOUNT NREADS_RAW"
    shell:
        "ParseHeaders.py table -s {input} -f {params.logfields} &>> {log}"

rule presto_count_collapsed:
    """Count reads and sequences from collapsed IgH sequences."""
    input: os.path.join(out_dir_pmerge, "R12_merged_collapse-unique_headers.tab")
    output: os.path.join(out_dir_pcount, "count-collapsed.tsv")
    log: os.path.join(log_dir_pcount, "count-collapsed.dbg")
    threads: config["threads"]["min"]
    params:
        stage = "presto_collapsed",
        cluster_barcodes = config["presto_params"]["cluster_barcodes_pident"],
        cluster_sets = config["presto_params"]["cluster_sets_pident"],
        group_field = config["count_on"],
        aux = aux_dir_preprocess,
    conda: env_r_preprocess
    script: os.path.join(script_dir_preprocess, "count_changeo_db.R")
#/////////////////////////////////////////////////////////////////////////////#

#-----------------------------------------------------------------------------
# Discard singletons
#-----------------------------------------------------------------------------

out_dir_poutput = os.path.join(out_dir_presto, "output")
log_dir_poutput = os.path.join(log_dir_presto, "output")

rule presto_split_seq:
    """Split off singleton seqs into separate file"""
    input: os.path.join(out_dir_pmerge, "R12_merged_collapse-unique.fastq")
    output:  os.path.join(out_dir_poutput, "R12_merged_atleast-2.fastq")
    params:
        prefix = lambda wildcards, output: \
            os.path.abspath("_".join(output[0].split("_")[:-1])),
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    log: os.path.join(log_dir_pmerge, "R12_merged_split_seq.dbg")
    shell:
        "SplitSeq.py group -f CONSCOUNT --num 2 "
        "-s {input} --outname {params.prefix} &> {log} "

#/////////////////////////////////////////////////////////////////////////////#
rule presto_data_table_split:
    """Create a data table from split IgH sequences."""
    input:  os.path.join(out_dir_poutput, "R12_merged_atleast-2.fastq")
    output:  os.path.join(out_dir_poutput, "R12_merged_atleast-2_headers.tab")
    conda: env_main_preprocess
    log: os.path.join(log_dir_poutput, "R1_data_table_split.dbg")
    threads: config["threads"]["min"]
    params:
        logfields = "SAMPLE REPLICATE INDIVIDUAL CONSCOUNT NREADS_RAW"
    shell:
        "ParseHeaders.py table -s {input} -f {params.logfields} &>> {log}"

rule presto_count_split:
    """Count reads and sequences from split IgH sequences."""
    input:  os.path.join(out_dir_poutput, "R12_merged_atleast-2_headers.tab")
    output: os.path.join(out_dir_pcount, "count-split.tsv")
    log: os.path.join(log_dir_pcount, "count-split.dbg")
    threads: config["threads"]["min"]
    params:
        stage = "presto_split",
        cluster_barcodes = config["presto_params"]["cluster_barcodes_pident"],
        cluster_sets = config["presto_params"]["cluster_sets_pident"],
        group_field = config["count_on"],
        aux = aux_dir_preprocess,
    conda: env_r_preprocess
    script: os.path.join(script_dir_preprocess, "count_changeo_db.R")
#/////////////////////////////////////////////////////////////////////////////#

#-----------------------------------------------------------------------------
# Format sequences for downstream processing
#-----------------------------------------------------------------------------

rule presto_make_fasta:
    """Convert non-singleton FASTQ file to FASTA format"""
    input: os.path.join(out_dir_poutput, "R12_merged_atleast-2.fastq"),
    output:  os.path.join(out_dir_poutput, "R12_merged_atleast-2.fasta")
    conda: env_main_preprocess
    threads: config["threads"]["min"]
    log: os.path.join(log_dir_pmerge, "R12_merged_make_fasta.dbg")
    shell: "seqtk seq -a {input} > {output} 2> {log}"

#-----------------------------------------------------------------------------
# Combine count reports over different pRESTO pipeline stages
#-----------------------------------------------------------------------------

rule presto_count_collate:
    """Collate count reports from different stages into a single table."""
    input:
        os.path.join(out_dir_pcount, "count-raw.tsv"),
        os.path.join(out_dir_pcount, "count-filtered.tsv"),
        os.path.join(out_dir_pcount, "count-masked.tsv"),
        os.path.join(out_dir_pcount, "count-corrected.tsv"),
        os.path.join(out_dir_pcount, "count-consensus.tsv"),
        os.path.join(out_dir_pcount, "count-merged.tsv"),
        os.path.join(out_dir_pcount, "count-collapsed.tsv"),
        os.path.join(out_dir_pcount, "count-split.tsv"),
    output: os.path.join(out_dir_pcount, "report.tsv")
    log: os.path.join(log_dir_pcount, "collate-counts.dbg")
    conda: env_r_preprocess
    params:
        aux = aux_dir_preprocess,
    script: os.path.join(script_dir_preprocess, "merge_dbs.R")
